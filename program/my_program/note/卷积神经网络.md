## 从全连接层到卷积

### 不变性

多输出通道：为每一个通道都有一个自己的三维卷积核

输入输出没有太多的相关性

+ 每个输出通道可以识别特定的模式

  <img src="X:\机器学习\program\my_program\note\image-20230311170859946.png" alt="image-20230311170859946" style="zoom:50%;" />

- 输入通道**核**识别并组合输入中的模式
- 输出通道数是卷积层的超参数
- 每个输入通道有独立的二维卷积核，所有通道结果相加得到一个输出通道结果
- 每个输出通道有独立的三位卷积核

### 二维卷积层

+ 输入**X**：$c_{i}*n_{h}*n_{w}$
+ 核**W**：$c_{o}*c_{i}*k_{h}*k_{w}$
+ 偏差**B**：$c_{o}*c_{i}$
+ 输出**Y**：$c_{o}*m_{h}*m_{w}$

​																	$$Y=X⭐W+B$$

2023.3.21

###### *多输出通道*

###### *理解每一步细节*

CNN EXPLAINER

微调

寻找先验知识(idea)

**dropout**

特征提取？？？视觉、序列... 原始——>最后 AlexNet 

## 批量归一化

- 加速深层神经网络的收敛速度，但一般不会改变模型的精度。
- BN固定小批量中的均值和方差，然后学习出适合的偏移和缩放。

![image-20230327170621436](X:\机器学习\program\my_program\note\image-20230327170621436.png)

![image-20230327170705328](X:\机器学习\program\my_program\note\image-20230327170705328.png)

#### 作用在

- 全连接和卷积层的输出上，激活函数前

- 全连接层和卷积层的输入上

  - 对全连接层，作用在特征维（每一列）
  - 对于卷积层，作用在通道维

  ##### 部分代码

  ```python
  def batch_norm(X,gamma,beta,moving_mean,moving_var,eps,momentum):
      if not torch.is_grad_enabled():
          #预测模式
          X_hat = (X - moving_mean) / torch.sqrt(moving_var + eps)
      else:
          assert len(X.shape) in (2,4)
          if len(X.shape) == 2:
              #全连接
              mean = X.mean(dim=0)
              var = ((X-mean)**2).mean(dim=0)
          else:
              #二维卷积
              #保持X的形状
              mean = X.mean(dim=(0,2,3),keepdim=True)
              var = ((X-mean)**2).mean(dim=(0,2,3),keepdim=True)
          #训练模式下，用当前的均值和方差做标准化
          X_hat = (X - mean) / torch.sqrt(var+eps)
          #更新移动平均的均值和方差
          moving_mean = momentum*moving_mean + (1.0-momentum) * mean
          moving_var = momentum*moving_var + (1.0-momentum) * var
      Y = gamma*X_hat + beta
      return Y,moving_mean.data,moving_var.data
  
  ```

  `hah`

#### 批量归一化在做什么？

- 最初论文是想减少内部协变量转移
- 后续由论文指出它可能就是通过在每个小批量里面加入噪声来控制模型的复杂度

<img src="X:\机器学习\program\my_program\note\image-20230327171847136.png" alt="image-20230327171847136" style="zoom:33%;" />

- 因此没必要跟dropout混合使用

## 残差网络ResNet

- 加更多的层总是改进精度嘛？

<img src="X:\机器学习\program\my_program\note\image-20230327210929495.png" alt="image-20230327210929495" style="zoom:80%;" />

- 残差块

​		-串联一个层会改变函数类，而我们希望**扩大**函数类

​		-残差块加入快速通道（右边）来得到$f(x)=x+g(x)$的结构。

<img src="X:\机器学习\program\my_program\note\image-20230327211309317.png" alt="image-20230327211309317" style="zoom:67%;" />

- ResNet块细节

<img src="X:\机器学习\program\my_program\note\image-20230327211810990.png" alt="image-20230327211810990" style="zoom:50%;" />
